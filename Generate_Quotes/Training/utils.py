import os, tkinter, tkinter.filedialog, tkinter.messagebox
import dataset
import pandas as pd
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.callbacks import EarlyStopping
from keras.models import Sequential
import keras.utils as ku
import pandas as pd
import numpy as np
import string, os
import warnings
from tqdm import tqdm

def extract_texts_from_db(db_path):
    """Summary line.

    ・databaseからtextのリストを取り出す

    Args:

    Returns:
        all_texts : textのlist
    """

    #---------------------------------------------
    # database の取得
    db = dataset.connect("sqlite:///"+db_path)

    #----------------------------------------
    #データベースから欲しいデータを取得する

    print("This database contains following tables",
          db.tables)
    table_name = input("specify table_name: ")
    df = pd.DataFrame(db[table_name])

    print("This dataframe containf following columns",
          df.columns)
    col_name  = input("specify column name: ")
    all_texts = list(df[col_name].values)

    return all_texts


def get_sequence_of_tokens(torknizer, corpus):
    """Summary line.

    ・torknizerと辞書をもとに、tokenizeをする

    Args:
        torlnizer    : keras_preprocessing.text.Tokenizer
        corpus (list): 学習させたいdocumentのリスト

    Returns:
        input_sequences : tokenizeされたdocumentのリスト
        totoal_words    : corpus全体のユニークな単語数

    example of returns :
        input_sequences[:4]
        [[33, 79],
         [33, 79, 25],
         [33, 79, 25, 53],
         [33, 79, 25, 53, 21],

    """
    assert type(corpus) == list, "corpusの型はlistです"
    assert len(corpus)  != 0, "corpusに中身がありません"

    print("fitiing torknizer on given corpus")
    torknizer.fit_on_texts(corpus)
    total_words = len(torknizer.word_index) + 1

    input_sequences = []

    print("converting given texts to sequences")
    for line in tqdm(corpus):
        token_list = torknizer.texts_to_sequences([line])[0]
        for i in range(1, len(token_list)):
            n_gram_sequence = token_list[:i+1]
            input_sequences.append(n_gram_sequence)

    return input_sequences, total_words

# pad sequences
def generate_padded_sequences(input_sequences, num_unique_words, max_sequence_len = None):
    """Summary line.

    ・torknizeされたsequencesをパディングして学習用データを作成
    ・パディング自体はkerasのものを使用

    Args:
        input_sequences (list) : sequences generated by get_sequence_of_tokens
        max_sequence_len (int) : パディング後のsequenceの長さの最大値
        num_unique_words       : 学習させるコーパスの単語数
    Returns:
        predictors       : 各sequenceの最後の一字を除いたもの、教師データ用
        label            : 各sequenceの最後の一字、教師ラベル用
        max_sequence_len :

    """

    assert type(input_sequences) == list, "input_sequencesの型はlist"

    if max_sequence_len == None:
        max_sequence_len = max([len(x) for x in input_sequences])

    input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))
    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]
    label = ku.to_categorical(label, num_classes = num_unique_words)

    return predictors, label, max_sequence_len

def create_model(max_sequence_len, total_words):
    """Summary line.

    ・学習用モデルを生成
    ・モデルを変更したいときには、この関数をいじること

    Args:
        max_sequence_len (int) : パディング後のsequenceの長さの最大値
        total_words      (int) : 学習に使うデータに含まれるユニークな単語の数
        epoch            (int) : epochの数

    Returns:
        model       : 学習用のモデル

    """

    # モデル定義
    model = Sequential()
    # Add Input Embedding Layer
    model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))
    # Add Hidden Layer 1 - LSTM Layer
    model.add(LSTM(100))
    model.add(Dropout(0.1))
    # Add Output Layer
    model.add(Dense(total_words, activation='softmax'))

    #-----------------------
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    print(model.summary())

    return model
